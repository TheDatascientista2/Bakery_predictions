{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cb84d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.9.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.5.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.21.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: statsmodels in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (0.14.4)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.13.1)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from statsmodels) (2.0.2)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from statsmodels) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[K     |████████████████████████████████| 307 kB 46.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/edilbekabdyrakhmanov/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.6.1 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "   %pip install seaborn\n",
    "   %pip install statsmodels\n",
    "   %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6068291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70688087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7517, 15)\n",
      "Validation shape: (1839, 15)\n",
      "Test shape: (351, 15)\n",
      "Starting Bakery Sales Analysis...\n",
      "==================================================\n",
      "=== Processing Training Data ===\n",
      "Removed columns: ['Wettercode', 'AverageTemp', 'Temp_vs_Avg']\n",
      "Added time-based features:\n",
      "- Month, Day_of_Year, Week_of_Year, Quarter, Year\n",
      "- Cyclical encodings: Month_sin/cos, Day_sin/cos, Week_sin/cos\n",
      "Encoded Weekday: ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday']\n",
      "Encoded Weather_Impression: ['bad', 'good', 'okay', 'very bad', 'very good']\n",
      "Filled 0 missing values in KielerWoche with median: 1.00\n",
      "Final feature matrix shape: (7487, 19)\n",
      "Target variable shape: (7487,)\n",
      "Features used: ['Bewoelkung', 'Temperatur', 'Windgeschwindigkeit', 'Temp_Deviation', 'Is_Holiday', 'KielerWoche', 'Month', 'Day_of_Year', 'Week_of_Year', 'Quarter', 'Year', 'Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Week_sin', 'Week_cos', 'Weekday_encoded', 'Weather_Impression_encoded']\n",
      "\n",
      "=== Processing Validation Data ===\n",
      "Removed columns: ['Wettercode', 'AverageTemp', 'Temp_vs_Avg']\n",
      "Added time-based features:\n",
      "- Month, Day_of_Year, Week_of_Year, Quarter, Year\n",
      "- Cyclical encodings: Month_sin/cos, Day_sin/cos, Week_sin/cos\n",
      "Encoded Weekday: ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday']\n",
      "Encoded Weather_Impression: ['bad', 'good', 'okay', 'very bad', 'very good']\n",
      "Filled 0 missing values in Bewoelkung with median: 6.00\n",
      "Filled 0 missing values in KielerWoche with median: 1.00\n",
      "Final feature matrix shape: (1831, 19)\n",
      "Target variable shape: (1831,)\n",
      "Features used: ['Bewoelkung', 'Temperatur', 'Windgeschwindigkeit', 'Temp_Deviation', 'Is_Holiday', 'KielerWoche', 'Month', 'Day_of_Year', 'Week_of_Year', 'Quarter', 'Year', 'Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Week_sin', 'Week_cos', 'Weekday_encoded', 'Weather_Impression_encoded']\n",
      "\n",
      "=== Processing Test Data ===\n",
      "Removed columns: ['Wettercode', 'AverageTemp', 'Temp_vs_Avg']\n",
      "Added time-based features:\n",
      "- Month, Day_of_Year, Week_of_Year, Quarter, Year\n",
      "- Cyclical encodings: Month_sin/cos, Day_sin/cos, Week_sin/cos\n",
      "Encoded Weekday: ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday']\n",
      "Encoded Weather_Impression: ['bad', 'good', 'okay', 'very bad', 'very good']\n",
      "Final feature matrix shape: (0, 19)\n",
      "Target variable shape: (0,)\n",
      "Features used: ['Bewoelkung', 'Temperatur', 'Windgeschwindigkeit', 'Temp_Deviation', 'Is_Holiday', 'KielerWoche', 'Month', 'Day_of_Year', 'Week_of_Year', 'Quarter', 'Year', 'Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Week_sin', 'Week_cos', 'Weekday_encoded', 'Weather_Impression_encoded']\n",
      "Training Linear Regression...\n",
      "Training Random Forest...\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "Linear Regression:\n",
      "  Training   - RMSE: 141.80, R²: 0.079, MAE: 110.09\n",
      "  Validation - RMSE: 125.35, R²: 0.074, MAE: 94.31\n",
      "\n",
      "Random Forest:\n",
      "  Training   - RMSE: 133.51, R²: 0.184, MAE: 105.92\n",
      "  Validation - RMSE: 120.93, R²: 0.138, MAE: 95.98\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "----------------------------------------\n",
      " 1. Temperatur                0.1867\n",
      " 2. Day_cos                   0.1230\n",
      " 3. Day_of_Year               0.1096\n",
      " 4. Weekday_encoded           0.1042\n",
      " 5. Temp_Deviation            0.0929\n",
      " 6. Windgeschwindigkeit       0.0783\n",
      " 7. Day_sin                   0.0728\n",
      " 8. Year                      0.0615\n",
      " 9. Bewoelkung                0.0554\n",
      "10. Week_of_Year              0.0213\n",
      "11. Weather_Impression_encoded 0.0195\n",
      "12. Week_sin                  0.0192\n",
      "13. Month_cos                 0.0190\n",
      "14. Week_cos                  0.0169\n",
      "15. Is_Holiday                0.0105\n"
     ]
    }
   ],
   "source": [
    "#Load your dataset\n",
    "df = pd.read_csv(\"/Users/edilbekabdyrakhmanov/Documents/GitHub/bakeryy/0_DataPreparation/initialdata/merged_data_temperature+holidays+weather_impressions.csv\") # \n",
    "\n",
    "#Ensure the 'Datum' column is in datetime format\n",
    "df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "#Define time ranges\n",
    "train_start = '2013-07-01'\n",
    "train_end = '2017-07-31'\n",
    "val_start = '2017-08-01'\n",
    "val_end = '2018-07-31'\n",
    "test_start = '2018-08-01'\n",
    "test_end = '2019-07-30'\n",
    "\n",
    "#Split data\n",
    "train_data = df[(df['Datum'] >= train_start) & (df['Datum'] <= train_end)]\n",
    "validation_data = df[(df['Datum'] >= val_start) & (df['Datum'] <= val_end)]\n",
    "test_data = df[(df['Datum'] >= test_start) & (df['Datum'] <= test_end)]\n",
    "\n",
    "#Optional: print shapes\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Validation shape:\", validation_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "\n",
    "# Load and prepare the data (assuming your existing split)\n",
    "def prepare_bakery_data(df):\n",
    "    \"\"\"\n",
    "    Prepare bakery data by removing problematic variables and adding time-based features\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Remove variables we decided to handle carefully\n",
    "    columns_to_remove = ['Wettercode', 'AverageTemp', 'Temp_vs_Avg']\n",
    "    existing_cols_to_remove = [col for col in columns_to_remove if col in data.columns]\n",
    "    if existing_cols_to_remove:\n",
    "        data = data.drop(columns=existing_cols_to_remove)\n",
    "        print(f\"Removed columns: {existing_cols_to_remove}\")\n",
    "    \n",
    "    # Ensure Datum is datetime\n",
    "    data['Datum'] = pd.to_datetime(data['Datum'])\n",
    "    \n",
    "    # Add time-based features\n",
    "    data['Month'] = data['Datum'].dt.month\n",
    "    data['Day_of_Year'] = data['Datum'].dt.dayofyear\n",
    "    data['Week_of_Year'] = data['Datum'].dt.isocalendar().week\n",
    "    data['Quarter'] = data['Datum'].dt.quarter\n",
    "    data['Year'] = data['Datum'].dt.year\n",
    "    \n",
    "    # Add cyclical encoding for better temporal representation\n",
    "    # Month (12 months cycle)\n",
    "    data['Month_sin'] = np.sin(2 * np.pi * data['Month'] / 12)\n",
    "    data['Month_cos'] = np.cos(2 * np.pi * data['Month'] / 12)\n",
    "    \n",
    "    # Day of year (365 day cycle)\n",
    "    data['Day_sin'] = np.sin(2 * np.pi * data['Day_of_Year'] / 365)\n",
    "    data['Day_cos'] = np.cos(2 * np.pi * data['Day_of_Year'] / 365)\n",
    "    \n",
    "    # Week of year (52 week cycle)\n",
    "    data['Week_sin'] = np.sin(2 * np.pi * data['Week_of_Year'] / 52)\n",
    "    data['Week_cos'] = np.cos(2 * np.pi * data['Week_of_Year'] / 52)\n",
    "    \n",
    "    print(\"Added time-based features:\")\n",
    "    print(\"- Month, Day_of_Year, Week_of_Year, Quarter, Year\")\n",
    "    print(\"- Cyclical encodings: Month_sin/cos, Day_sin/cos, Week_sin/cos\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def encode_categorical_variables(data):\n",
    "    \"\"\"\n",
    "    Encode categorical variables for modeling\n",
    "    \"\"\"\n",
    "    # Make a copy\n",
    "    data_encoded = data.copy()\n",
    "    \n",
    "    # Label encode categorical variables\n",
    "    label_encoders = {}\n",
    "    categorical_cols = ['Weekday', 'Weather_Impression']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in data_encoded.columns:\n",
    "            le = LabelEncoder()\n",
    "            # Handle missing values\n",
    "            data_encoded[col] = data_encoded[col].fillna('Unknown')\n",
    "            data_encoded[f'{col}_encoded'] = le.fit_transform(data_encoded[col])\n",
    "            label_encoders[col] = le\n",
    "            print(f\"Encoded {col}: {list(le.classes_)}\")\n",
    "    \n",
    "    return data_encoded, label_encoders\n",
    "\n",
    "def prepare_features_and_target(data):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix and target variable, handling missing values\n",
    "    \"\"\"\n",
    "    # Define feature columns (excluding target and identifier columns)\n",
    "    feature_cols = [\n",
    "        'Bewoelkung', 'Temperatur', 'Windgeschwindigkeit', \n",
    "        'Temp_Deviation', 'Is_Holiday', 'KielerWoche',\n",
    "        'Month', 'Day_of_Year', 'Week_of_Year', 'Quarter', 'Year',\n",
    "        'Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Week_sin', 'Week_cos',\n",
    "        'Weekday_encoded', 'Weather_Impression_encoded'\n",
    "    ]\n",
    "    \n",
    "    # Filter only existing columns\n",
    "    available_features = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "    # Remove rows where target variable (Umsatz) is missing\n",
    "    data_clean = data.dropna(subset=['Umsatz']).copy()\n",
    "    \n",
    "    # Prepare feature matrix\n",
    "    X = data_clean[available_features].copy()\n",
    "    y = data_clean['Umsatz'].copy()\n",
    "    \n",
    "    # Handle missing values in features\n",
    "    # For numerical columns, use median\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    for col in numerical_cols:\n",
    "        if X[col].isnull().any():\n",
    "            median_val = X[col].median()\n",
    "            X[col] = X[col].fillna(median_val)\n",
    "            print(f\"Filled {X[col].isnull().sum()} missing values in {col} with median: {median_val:.2f}\")\n",
    "    \n",
    "    # For KielerWoche, fill with 0 (assuming it's binary)\n",
    "    if 'KielerWoche' in X.columns:\n",
    "        X['KielerWoche'] = X['KielerWoche'].fillna(0)\n",
    "    \n",
    "    print(f\"Final feature matrix shape: {X.shape}\")\n",
    "    print(f\"Target variable shape: {y.shape}\")\n",
    "    print(f\"Features used: {list(X.columns)}\")\n",
    "    \n",
    "    return X, y, data_clean\n",
    "\n",
    "# Example usage with your existing data splits\n",
    "def process_bakery_data(train_data, validation_data, test_data):\n",
    "    \"\"\"\n",
    "    Process all data splits consistently\n",
    "    \"\"\"\n",
    "    print(\"=== Processing Training Data ===\")\n",
    "    train_processed = prepare_bakery_data(train_data)\n",
    "    train_encoded, label_encoders = encode_categorical_variables(train_processed)\n",
    "    X_train, y_train, train_clean = prepare_features_and_target(train_encoded)\n",
    "    \n",
    "    print(\"\\n=== Processing Validation Data ===\")\n",
    "    val_processed = prepare_bakery_data(validation_data)\n",
    "    val_encoded, _ = encode_categorical_variables(val_processed)\n",
    "    # Apply same encoders as training data\n",
    "    for col, le in label_encoders.items():\n",
    "        if col in val_encoded.columns:\n",
    "            val_encoded[f'{col}_encoded'] = val_encoded[col].map(\n",
    "                dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            ).fillna(-1)  # Unknown categories get -1\n",
    "    \n",
    "    X_val, y_val, val_clean = prepare_features_and_target(val_encoded)\n",
    "    \n",
    "    print(\"\\n=== Processing Test Data ===\")\n",
    "    test_processed = prepare_bakery_data(test_data)\n",
    "    test_encoded, _ = encode_categorical_variables(test_processed)\n",
    "    # Apply same encoders as training data\n",
    "    for col, le in label_encoders.items():\n",
    "        if col in test_encoded.columns:\n",
    "            test_encoded[f'{col}_encoded'] = test_encoded[col].map(\n",
    "                dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            ).fillna(-1)  # Unknown categories get -1\n",
    "    \n",
    "    X_test, y_test, test_clean = prepare_features_and_target(test_encoded)\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'label_encoders': label_encoders,\n",
    "        'train_clean': train_clean,\n",
    "        'val_clean': val_clean,\n",
    "        'test_clean': test_clean\n",
    "    }\n",
    "\n",
    "# Simple regression models\n",
    "def train_regression_models(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train and evaluate regression models\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Linear Regression\n",
    "    print(\"Training Linear Regression...\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    models['Linear Regression'] = lr\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = lr.predict(X_train)\n",
    "    y_val_pred = lr.predict(X_val)\n",
    "    \n",
    "    # Metrics\n",
    "    results['Linear Regression'] = {\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'val_rmse': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        'train_r2': r2_score(y_train, y_train_pred),\n",
    "        'val_r2': r2_score(y_val, y_val_pred),\n",
    "        'train_mae': mean_absolute_error(y_train, y_train_pred),\n",
    "        'val_mae': mean_absolute_error(y_val, y_val_pred)\n",
    "    }\n",
    "    \n",
    "    # 2. Random Forest (for comparison)\n",
    "    print(\"Training Random Forest...\")\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    models['Random Forest'] = rf\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred_rf = rf.predict(X_train)\n",
    "    y_val_pred_rf = rf.predict(X_val)\n",
    "    \n",
    "    # Metrics\n",
    "    results['Random Forest'] = {\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred_rf)),\n",
    "        'val_rmse': np.sqrt(mean_squared_error(y_val, y_val_pred_rf)),\n",
    "        'train_r2': r2_score(y_train, y_train_pred_rf),\n",
    "        'val_r2': r2_score(y_val, y_val_pred_rf),\n",
    "        'train_mae': mean_absolute_error(y_train, y_train_pred_rf),\n",
    "        'val_mae': mean_absolute_error(y_val, y_val_pred_rf)\n",
    "    }\n",
    "    \n",
    "    return models, results\n",
    "\n",
    "def print_model_results(results):\n",
    "    \"\"\"\n",
    "    Print model performance results in a nice format\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Training   - RMSE: {metrics['train_rmse']:.2f}, R²: {metrics['train_r2']:.3f}, MAE: {metrics['train_mae']:.2f}\")\n",
    "        print(f\"  Validation - RMSE: {metrics['val_rmse']:.2f}, R²: {metrics['val_r2']:.3f}, MAE: {metrics['val_mae']:.2f}\")\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, top_n=15):\n",
    "    \"\"\"\n",
    "    Analyze feature importance for tree-based models\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop {top_n} Most Important Features:\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, (_, row) in enumerate(importance_df.head(top_n).iterrows()):\n",
    "            print(f\"{i+1:2d}. {row['feature']:<25} {row['importance']:.4f}\")\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"Model doesn't have feature importance attribute\")\n",
    "        return None\n",
    "\n",
    "# Complete workflow function\n",
    "def run_bakery_analysis(train_data, validation_data, test_data):\n",
    "    \"\"\"\n",
    "    Run complete analysis workflow\n",
    "    \"\"\"\n",
    "    print(\"Starting Bakery Sales Analysis...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Process data\n",
    "    processed_data = process_bakery_data(train_data, validation_data, test_data)\n",
    "    \n",
    "    # Train models\n",
    "    models, results = train_regression_models(\n",
    "        processed_data['X_train'], processed_data['y_train'],\n",
    "        processed_data['X_val'], processed_data['y_val']\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print_model_results(results)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    if 'Random Forest' in models:\n",
    "        importance_df = analyze_feature_importance(\n",
    "            models['Random Forest'], \n",
    "            processed_data['X_train'].columns\n",
    "        )\n",
    "    \n",
    "    return processed_data, models, results\n",
    "\n",
    "# To run the complete analysis, use:\n",
    "# processed_data, models, results = run_bakery_analysis(train_data, validation_data, test_data)\n",
    "processed_data, models, results = run_bakery_analysis(train_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ddd7f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Enhanced OLS Analysis...\n",
      "============================================================\n",
      "ENHANCED OLS REGRESSION ANALYSIS\n",
      "============================================================\n",
      "Building feature matrix...\n",
      "Added Warengruppe dummies: ['Warengruppe_Brötchen', 'Warengruppe_Croissant', 'Warengruppe_Konditorei', 'Warengruppe_Kuchen', 'Warengruppe_Saisonbrot']\n",
      "Added Weekday dummies: ['Weekday_Monday', 'Weekday_Saturday', 'Weekday_Sunday', 'Weekday_Thursday', 'Weekday_Tuesday', 'Weekday_Wednesday']\n",
      "Added Weather dummies: ['Weather_good', 'Weather_okay', 'Weather_very bad', 'Weather_very good']\n",
      "Added continuous variable: Temperatur\n",
      "Added continuous variable: Windgeschwindigkeit\n",
      "Added continuous variable: Bewoelkung\n",
      "Added continuous variable: Temp_Deviation\n",
      "Added binary variable: Is_Holiday\n",
      "Added binary variable: KielerWoche\n",
      "Added time feature: Month\n",
      "Added time feature: Quarter\n",
      "Added time feature: Month_sin\n",
      "Added time feature: Month_cos\n",
      "Added time feature: Day_sin\n",
      "Added time feature: Day_cos\n",
      "Added time feature: Week_sin\n",
      "Added time feature: Week_cos\n",
      "\n",
      "Final feature matrix shape: (7517, 30)\n",
      "Features: ['const', 'Warengruppe_Brötchen', 'Warengruppe_Croissant', 'Warengruppe_Konditorei', 'Warengruppe_Kuchen', 'Warengruppe_Saisonbrot', 'Weekday_Monday', 'Weekday_Saturday', 'Weekday_Sunday', 'Weekday_Thursday', 'Weekday_Tuesday', 'Weekday_Wednesday', 'Weather_good', 'Weather_okay', 'Weather_very bad', 'Weather_very good', 'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', 'Temp_Deviation', 'Is_Holiday', 'KielerWoche', 'Month', 'Quarter', 'Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'Week_sin', 'Week_cos']\n",
      "\n",
      "Data after cleaning:\n",
      "Observations: 7487\n",
      "Features: 30 (including constant)\n",
      "\n",
      "Fitting OLS model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 184\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Run the analysis\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Enhanced OLS Analysis...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 184\u001b[0m results, Y_clean, X_clean, processed_data \u001b[38;5;241m=\u001b[39m \u001b[43menhanced_ols_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 143\u001b[0m, in \u001b[0;36menhanced_ols_analysis\u001b[0;34m(train_data)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Build and fit the model\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m results, Y_clean, X_clean, processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_ols_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Print the summary (same as teammate)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOLS REGRESSION RESULTS:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 129\u001b[0m, in \u001b[0;36mbuild_ols_model\u001b[0;34m(train_data)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Fit the OLS model - same as teammate\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFitting OLS model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, Y_clean, X_clean, data\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/statsmodels/regression/linear_model.py:921\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    918\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    919\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    920\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/statsmodels/regression/linear_model.py:746\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    745\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 746\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    749\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/statsmodels/regression/linear_model.py:200\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/statsmodels/base/model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/statsmodels/base/model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/statsmodels/base/model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/statsmodels/base/data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/statsmodels/base/data.py:84\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_endog \u001b[38;5;241m=\u001b[39m endog\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_exog \u001b[38;5;241m=\u001b[39m exog\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_endog_exog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/statsmodels/base/data.py:509\u001b[0m, in \u001b[0;36mPandasData._convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    507\u001b[0m exog \u001b[38;5;241m=\u001b[39m exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas data cast to numpy dtype of object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck input data with np.asarray(data).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_convert_endog_exog(endog, exog)\n",
      "\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_ols_data(train_data):\n",
    "    \"\"\"\n",
    "    Prepare data for OLS regression with enhanced features\n",
    "    \"\"\"\n",
    "    # Make a copy\n",
    "    data = train_data.copy()\n",
    "    \n",
    "    # Ensure Datum is datetime\n",
    "    data['Datum'] = pd.to_datetime(data['Datum'])\n",
    "    \n",
    "    # Add time-based features (same as before)\n",
    "    data['Month'] = data['Datum'].dt.month\n",
    "    data['Day_of_Year'] = data['Datum'].dt.dayofyear\n",
    "    data['Week_of_Year'] = data['Datum'].dt.isocalendar().week\n",
    "    data['Quarter'] = data['Datum'].dt.quarter\n",
    "    data['Year'] = data['Datum'].dt.year\n",
    "    \n",
    "    # Add cyclical encoding for better temporal representation\n",
    "    data['Month_sin'] = np.sin(2 * np.pi * data['Month'] / 12)\n",
    "    data['Month_cos'] = np.cos(2 * np.pi * data['Month'] / 12)\n",
    "    data['Day_sin'] = np.sin(2 * np.pi * data['Day_of_Year'] / 365)\n",
    "    data['Day_cos'] = np.cos(2 * np.pi * data['Day_of_Year'] / 365)\n",
    "    data['Week_sin'] = np.sin(2 * np.pi * data['Week_of_Year'] / 52)\n",
    "    data['Week_cos'] = np.cos(2 * np.pi * data['Week_of_Year'] / 52)\n",
    "    \n",
    "    # Fill missing values\n",
    "    data['KielerWoche'] = data['KielerWoche'].fillna(0)\n",
    "    data['Is_Holiday'] = data['Is_Holiday'].fillna(0)\n",
    "    data['Temp_Deviation'] = data['Temp_Deviation'].fillna(data['Temp_Deviation'].median())\n",
    "    data['Bewoelkung'] = data['Bewoelkung'].fillna(data['Bewoelkung'].median())\n",
    "    data['Weather_Impression'] = data['Weather_Impression'].fillna('Unknown')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def build_ols_model(train_data):\n",
    "    \"\"\"\n",
    "    Build OLS model with enhanced features following your teammate's approach\n",
    "    \"\"\"\n",
    "    # Prepare the data\n",
    "    data = prepare_ols_data(train_data)\n",
    "    \n",
    "    # Define target variable\n",
    "    Y = data['Umsatz']\n",
    "    \n",
    "    # Build feature matrix step by step (similar to your teammate's approach)\n",
    "    print(\"Building feature matrix...\")\n",
    "    \n",
    "    # Start with product categories (Warengruppe) - same as teammate\n",
    "    X_components = []\n",
    "    \n",
    "    # 1. Product categories (dummy variables)\n",
    "    if 'Warengruppe' in data.columns:\n",
    "        warengruppe_dummies = pd.get_dummies(data['Warengruppe'], \n",
    "                                           prefix='Warengruppe', \n",
    "                                           drop_first=True, \n",
    "                                           dtype=int)\n",
    "        X_components.append(warengruppe_dummies)\n",
    "        print(f\"Added Warengruppe dummies: {list(warengruppe_dummies.columns)}\")\n",
    "    \n",
    "    # 2. Weekday dummies - same as teammate\n",
    "    weekday_dummies = pd.get_dummies(data['Weekday'], \n",
    "                                   prefix='Weekday', \n",
    "                                   drop_first=True, \n",
    "                                   dtype=int)\n",
    "    X_components.append(weekday_dummies)\n",
    "    print(f\"Added Weekday dummies: {list(weekday_dummies.columns)}\")\n",
    "    \n",
    "    # 3. Weather impression dummies (our addition)\n",
    "    weather_dummies = pd.get_dummies(data['Weather_Impression'], \n",
    "                                   prefix='Weather', \n",
    "                                   drop_first=True, \n",
    "                                   dtype=int)\n",
    "    X_components.append(weather_dummies)\n",
    "    print(f\"Added Weather dummies: {list(weather_dummies.columns)}\")\n",
    "    \n",
    "    # 4. Continuous weather variables (enhanced from teammate's version)\n",
    "    continuous_vars = ['Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', 'Temp_Deviation']\n",
    "    for var in continuous_vars:\n",
    "        if var in data.columns:\n",
    "            X_components.append(data[var])\n",
    "            print(f\"Added continuous variable: {var}\")\n",
    "    \n",
    "    # 5. Binary variables\n",
    "    binary_vars = ['Is_Holiday', 'KielerWoche']\n",
    "    for var in binary_vars:\n",
    "        if var in data.columns:\n",
    "            X_components.append(data[var])\n",
    "            print(f\"Added binary variable: {var}\")\n",
    "    \n",
    "    # 6. Time-based features (our major enhancement)\n",
    "    time_features = ['Month', 'Quarter', 'Month_sin', 'Month_cos', \n",
    "                    'Day_sin', 'Day_cos', 'Week_sin', 'Week_cos']\n",
    "    for var in time_features:\n",
    "        X_components.append(data[var])\n",
    "        print(f\"Added time feature: {var}\")\n",
    "    \n",
    "    # Combine all components\n",
    "    X = pd.concat(X_components, axis=1)\n",
    "    \n",
    "    # Ensure all columns are numeric (float)\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Add constant (intercept) - same as teammate\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    print(f\"\\nFinal feature matrix shape: {X.shape}\")\n",
    "    print(f\"Features: {list(X.columns)}\")\n",
    "    \n",
    "    # Clean data (remove NaN) - same approach as teammate\n",
    "    valid_idx = Y.notna() & X.notna().all(axis=1)\n",
    "    Y_clean = Y[valid_idx]\n",
    "    X_clean = X[valid_idx]\n",
    "    \n",
    "    # Reset indices - same as teammate\n",
    "    Y_clean.index = range(len(Y_clean))\n",
    "    X_clean.index = range(len(X_clean))\n",
    "    \n",
    "    print(f\"\\nData after cleaning:\")\n",
    "    print(f\"Observations: {len(Y_clean)}\")\n",
    "    print(f\"Features: {X_clean.shape[1]} (including constant)\")\n",
    "    \n",
    "    # Fit the OLS model - same as teammate\n",
    "    print(\"\\nFitting OLS model...\")\n",
    "    model = sm.OLS(Y_clean, X_clean)\n",
    "    results = model.fit()\n",
    "    \n",
    "    return results, Y_clean, X_clean, data\n",
    "\n",
    "def enhanced_ols_analysis(train_data):\n",
    "    \"\"\"\n",
    "    Run the complete OLS analysis with enhanced features\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENHANCED OLS REGRESSION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Build and fit the model\n",
    "    results, Y_clean, X_clean, processed_data = build_ols_model(train_data)\n",
    "    \n",
    "    # Print the summary (same as teammate)\n",
    "    print(\"\\nOLS REGRESSION RESULTS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(results.summary())\n",
    "    \n",
    "    # Additional diagnostics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ADDITIONAL MODEL DIAGNOSTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"R-squared: {results.rsquared:.4f}\")\n",
    "    print(f\"Adjusted R-squared: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"F-statistic: {results.fvalue:.2f}\")\n",
    "    print(f\"F-statistic p-value: {results.f_pvalue:.2e}\")\n",
    "    print(f\"AIC: {results.aic:.2f}\")\n",
    "    print(f\"BIC: {results.bic:.2f}\")\n",
    "    \n",
    "    # Most significant features\n",
    "    print(f\"\\nMost Significant Features (p < 0.01):\")\n",
    "    print(\"-\" * 50)\n",
    "    significant_features = results.pvalues[results.pvalues < 0.01].sort_values()\n",
    "    for feature, pval in significant_features.head(15).items():\n",
    "        coef = results.params[feature]\n",
    "        print(f\"{feature:<30} Coef: {coef:8.3f}, p-value: {pval:.2e}\")\n",
    "    \n",
    "    # Feature importance by absolute coefficient value\n",
    "    print(f\"\\nLargest Coefficients (by absolute value):\")\n",
    "    print(\"-\" * 50)\n",
    "    abs_coefs = results.params.abs().sort_values(ascending=False)\n",
    "    for feature in abs_coefs.head(15).index:\n",
    "        if feature != 'const':  # Skip intercept\n",
    "            coef = results.params[feature]\n",
    "            pval = results.pvalues[feature]\n",
    "            print(f\"{feature:<30} Coef: {coef:8.3f}, p-value: {pval:.3f}\")\n",
    "    \n",
    "    return results, Y_clean, X_clean, processed_data\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Starting Enhanced OLS Analysis...\")\n",
    "results, Y_clean, X_clean, processed_data = enhanced_ols_analysis(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
